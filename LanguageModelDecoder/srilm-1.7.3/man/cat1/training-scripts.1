training-scripts(1)         General Commands Manual        training-scripts(1)



NNAAMMEE
       training-scripts,   compute-oov-rate,  continuous-ngram-count,  get-gt-
       counts, make-abs-discount,  make-batch-counts,  make-big-lm,  make-dia-
       critic-map,    make-google-ngrams,  make-gt-discounts,  make-kn-counts,
       make-kn-discounts,  merge-batch-counts,   replace-unk-words,   replace-
       words-with-classes, reverse-ngram-counts, split-tagged-ngrams, reverse-
       text, tolower-ngram-counts, uniform-classes, uniq-ngram-counts, vp2text
       - miscellaneous conveniences for language model training

SSYYNNOOPPSSIISS
       ggeett--ggtt--ccoouunnttss mmaaxx==_K oouutt==_n_a_m_e [ _c_o_u_n_t_s ... ] >> _g_t_c_o_u_n_t_s
       mmaakkee--aabbss--ddiissccoouunntt _g_t_c_o_u_n_t_s
       mmaakkee--ggtt--ddiissccoouunnttss mmiinn==_m_i_n mmaaxx==_m_a_x _g_t_c_o_u_n_t_s
       mmaakkee--kknn--ccoouunnttss oorrddeerr==_N mmaaxx__ppeerr__ffiillee==_M oouuttppuutt==_f_i_l_e \
            [ nnoo__mmaaxx__oorrddeerr==11 ] >> _c_o_u_n_t_s
       mmaakkee--kknn--ddiissccoouunnttss mmiinn==_m_i_n _g_t_c_o_u_n_t_s
       mmaakkee--bbaattcchh--ccoouunnttss _f_i_l_e_-_l_i_s_t \
            [ _b_a_t_c_h_-_s_i_z_e [ _f_i_l_t_e_r [ _c_o_u_n_t_-_d_i_r [ _o_p_t_i_o_n_s ... ] ] ] ]
       mmeerrggee--bbaattcchh--ccoouunnttss [ --ffllooaatt--ccoouunnttss ] [ --ll _N ] _c_o_u_n_t_-_d_i_r [ _f_i_l_e_-_l_i_s_t|_s_t_a_r_t_-_i_t_e_r ]
       mmaakkee--ggooooggllee--nnggrraammss [ ddiirr==_D_I_R ] [ ppeerr__ffiillee==_N ] [ ggzziipp==00 ] \
            [ yyaahhoooo==11 ] [ _c_o_u_n_t_s_-_f_i_l_e ... ]
       ccoonnttiinnuuoouuss--nnggrraamm--ccoouunntt [ oorrddeerr==_N ] [ _t_e_x_t_f_i_l_e ... ]
       ttoolloowweerr--nnggrraamm--ccoouunnttss [ _c_o_u_n_t_s_-_f_i_l_e ... ]
       uunniiqq--nnggrraamm--ccoouunnttss [ _c_o_u_n_t_s_-_f_i_l_e ... ]
       rreevveerrssee--nnggrraamm--ccoouunnttss [ _c_o_u_n_t_s_-_f_i_l_e ... ]
       rreevveerrssee--tteexxtt [ _t_e_x_t_f_i_l_e ... ]
       sspplliitt--ttaaggggeedd--nnggrraammss [ sseeppaarraattoorr==_S ] [ _c_o_u_n_t_s_-_f_i_l_e ... ]
       mmaakkee--bbiigg--llmm --nnaammee _n_a_m_e --rreeaadd _c_o_u_n_t_s --llmm _n_e_w_-_m_o_d_e_l \
            [ --ttrruusstt--ttoottaallss ] [ --mmaaxx--ppeerr--ffiillee _M ] [ --nnggrraamm--ffiilltteerr _f_i_l_t_e_r ] \
            [ --tteexxtt _f_i_l_e ]] [[ _n_g_r_a_m_-_o_p_t_i_o_n_s ...... ]]
       rreeppllaaccee--uunnkk--wwoorrddss vvooccaabb==_v_o_c_a_b [[ _t_e_x_t_f_i_l_e ...... ]]
       rreeppllaaccee--wwoorrddss--wwiitthh--ccllaasssseess ccllaasssseess==_c_l_a_s_s_e_s [[ oouuttffiillee==_c_o_u_n_t_s ]] \\
            [[ nnoorrmmaalliizzee==00||11 ]] [[ aaddddoonnee==_K ]] [[ hhaavvee__ccoouunnttss==11 ]] [[ ppaarrttiiaall==11 ]] \\
            [[ _t_e_x_t_f_i_l_e ...... ]]
       uunniiffoorrmm--ccllaasssseess _c_l_a_s_s_e_s >> _n_e_w_-_c_l_a_s_s_e_s
       mmaakkee--ddiiaaccrriittiicc--mmaapp _v_o_c_a_b
       vvpp22tteexxtt [[ _t_e_x_t_f_i_l_e ...... ]]
       ccoommppuuttee--oooovv--rraattee _v_o_c_a_b [[ _c_o_u_n_t_s ...... ]]

DDEESSCCRRIIPPTTIIOONN
       These scripts perform convenience tasks associated with the training of
       language models.  They complement and extend  the  basic  N-gram  model
       estimator in nnggrraamm--ccoouunntt(1).

       Since  these  tools are implemented as scripts they don't automatically
       input or output compressed data files correctly, unlike the main  SRILM
       tools.   However, since most scripts work with data from standard input
       or to standard output (by leaving out the file argument, or  specifying
       it  as  ``-'')  it is easy to combine them with gguunnzziipp(1) or ggzziipp(1) on
       the command line.

       Also note that many of the scripts take their options with the  ggaawwkk(1)
       syntax _o_p_t_i_o_n==_v_a_l_u_e instead of the more common --_o_p_t_i_o_n _v_a_l_u_e.

       ggeett--ggtt--ccoouunnttss  computes the counts-of-counts statistics needed in Good-
       Turing smoothing.  The frequencies of  counts  up  to  _K  are  computed
       (default is 10).  The results are stored in a series of files with root
       _n_a_m_e,  _n_a_m_e..ggtt11ccoouunnttss,  _n_a_m_e..ggtt22ccoouunnttss,  ...,  _n_a_m_e..ggtt_Nccoouunnttss.   It  is
       assumed  that  the  input  counts have been properly merged, i.e., that
       there are no duplicated N-grams.

       mmaakkee--ggtt--ddiissccoouunnttss takes one of the output files  of  ggeett--ggtt--ccoouunnttss  and
       computes the corresponding Good-Turing discounting factors.  The output
       can then be passed to nnggrraamm--ccoouunntt(1) via the --ggtt_n  options  to  control
       the smoothing during model estimation.  Precomputing the GT discounting
       in this fashion has the  advantage  that  the  GT  statistics  are  not
       affected by restricting N-grams to a limited vocabulary.  Also, ggeett--ggtt--
       ccoouunnttss/mmaakkee--ggtt--ddiissccoouunnttss can process  arbitrarily  large  count  files,
       since  they  do  not need to read the counts into memory (unlike nnggrraamm--
       ccoouunntt).

       mmaakkee--aabbss--ddiissccoouunntt computes the absolute discounting constant needed for
       the  nnggrraamm--ccoouunntt  --ccddiissccoouunntt_n  options.  Input is one of the files pro-
       duced by ggeett--ggtt--ccoouunnttss.

       mmaakkee--kknn--ddiissccoouunntt computes the discounting constants used by  the  modi-
       fied  Kneser-Ney  smoothing method.  Input is one of the files produced
       by ggeett--ggtt--ccoouunnttss.  This script also implements a method for extrapolat-
       ing missing counts of counts as described in Wang et al. (2007).

       mmaakkee--bbaattcchh--ccoouunnttss  performs the first stage in the construction of very
       large N-gram count files.  _f_i_l_e_-_l_i_s_t is a list  of  input  text  files.
       Lines  starting  with a `#' character are ignored.  These files will be
       grouped into batches of size _b_a_t_c_h_-_s_i_z_e (default 10) that are then pro-
       cessed in one run of nnggrraamm--ccoouunntt each.  For maximum performance, _b_a_t_c_h_-
       _s_i_z_e should be as large as possible without triggering paging.  Option-
       ally,  a  _f_i_l_t_e_r  script or program can be given to condition the input
       texts.   The  N-gram  count  files  are  left  in  directory  _c_o_u_n_t_-_d_i_r
       (``counts'' by default), where they can be found by a subsequent run of
       mmeerrggee--bbaattcchh--ccoouunnttss.  All following _o_p_t_i_o_n_s are passed  to  nnggrraamm--ccoouunntt,
       e.g., to control N-gram order, vocabulary, etc.  (no options triggering
       model estimation should be included).

       mmeerrggee--bbaattcchh--ccoouunnttss completes the construction of large count  files  by
       merging  the batched counts left in _c_o_u_n_t_-_d_i_r until a single count file
       is produced.  Optionally, a _f_i_l_e_-_l_i_s_t of count files to combine can  be
       specified;  otherwise  all count files in _c_o_u_n_t_-_d_i_r from a prior run of
       mmaakkee--bbaattcchh--ccoouunnttss will be merged.  A number as second argument restarts
       the  merging  process  at  iteration _s_t_a_r_t_-_i_t_e_r.  This is convenient if
       merging fails to complete for some reason (e.g., for temporary lack  of
       disk space).  The --ffllooaatt--ccoouunnttss option should be specific if the counts
       are real-valued.  The --ll option specifies the number of files to  merge
       in each iteration; the default is 2.

       mmaakkee--ggooooggllee--nnggrraammss  takes  a  sorted count file as input and creates an
       indexed directory structure, in a format developed by Google  to  store
       very  large  N-gram  collections.   The resulting directory can then be
       used with the nnggrraamm--ccoouunntt(1) --rreeaadd--ggooooggllee option.   Optional  arguments
       specify  the  output  directory _d_i_r and the size _N of individual N-gram
       files (default is 10 million N-grams  per  file).   The  ggzziipp==00  option
       writes  plain, as opposed to compressed, files.  The yyaahhoooo==11 option may
       be used to read N-gram count files in Yahoo-GALE format.  Note that the
       count  files  have  to  first be sorted lexicographically in a separate
       invocation of sort.

       ccoonnttiinnuuoouuss--nnggrraamm--ccoouunntt generates N-grams that span line  breaks  (which
       are  usually taken to be sentence boundaries).  To count N-grams across
       line breaks use
            continuous-ngram-count _t_e_x_t_f_i_l_e | ngram-count -read -
       The argument _N controls the order of N-grams counted (default  3),  and
       should match  the argument of nnggrraamm--ccoouunntt --oorrddeerr.

       ttoolloowweerr--nnggrraamm--ccoouunnttss  maps  an N-gram counts file to all-lowercase.  No
       merging of N-grams that become identical in the process is done.

       uunniiqq--nnggrraamm--ccoouunnttss combines successive counts that pertain to  the  same
       N-gram  into  a  single  line.  This only affects repeated N-grams that
       appear on successive lines, so a prior sort command is typically  used,
       e.g.,
            tolower-ngram-counts _I_N_P_U_T | sort | uniq-ngram-counts > OUTPUT
       would do much of the same thing as
            ngram-counts -read _I_N_P_U_T -tolower -sort -write _O_U_T_P_U_T
       but  in a more memory-efficient manner (without reading all counts into
       memory).

       rreevveerrssee--nnggrraamm--ccoouunnttss reverses the word order of  N-grams  in  a  counts
       file  or  stream.   For  example,  to recompute lower-order counts from
       higher-order ones, but do the summation over  preceding  words  (rather
       than following words, as in nnggrraamm--ccoouunntt(1)), use
            reverse-ngram-counts _c_o_u_n_t_-_f_i_l_e | \
            ngram-count -read - -recompute -write - | \
            reverse-ngram-counts > _n_e_w_-_c_o_u_n_t_s
       Also,  start-sentence  tags  are  replaced  with end-sentence tags, and
       vice-versa, so that reverse-direction LMs can be trained from  forward-
       direction N-gram counts.

       rreevveerrssee--tteexxtt  reverses  the  word  order  in  text files, line-by-line.
       Start- and end-sentence tags, if  present,  will  be  preserved.   This
       reversal  is  appropriate  for preprocessing training data for LMs that
       are meant to be used with the nnggrraamm --rreevveerrssee option.

       sspplliitt--ttaaggggeedd--nnggrraammss expands N-gram count of word/tag pairs  into  mixed
       N-grams  of  words  and tags.  The optional sseeppaarraattoorr==_S argument allows
       the delimiting character, which defaults to "/", to be modified.

       mmaakkee--bbiigg--llmm constructs large N-gram models in a  more  memory-efficient
       way  than  nnggrraamm--ccoouunntt by itself.  It does so by precomputing the Good-
       Turing or Kneser-Ney smoothing parameters from the full set of  counts,
       and  then  instructing nnggrraamm--ccoouunntt to store only a subset of the counts
       in memory, namely those of N-grams to be retained in  the  model.   The
       _n_a_m_e  parameter  is  used to name various auxiliary files.  _c_o_u_n_t_s con-
       tains the raw N-gram counts; it may be (and usually  is)  a  compressed
       file.   Unlike  with  nnggrraamm--ccoouunntt,  the --rreeaadd option can be repeated to
       concatenate multiple count files, but the  arguments  must  be  regular
       files;  reading  from stdin is not supported.  If Good-Turing smoothing
       is used and the file contains complete lower-order counts corresponding
       to  the sums of higher-order counts, then the --ttrruusstt--ttoottaallss options may
       be given for efficiency.  The --tteexxtt option  specifies  a  test  set  to
       which  the  LM  is  to be applied, and builds the LM in such a way that
       only N-gram context occurring in the test  data  are  included  in  the
       model,  this  saving  space  at  the  expense of generality.  All other
       _o_p_t_i_o_n_s are passed to nnggrraamm--ccoouunntt (only options affecting model estima-
       tion should be given).  Smoothing methods other than Good-Turing, modi-
       fied Kneser-Ney and  Witten-Bell  are  not  supported  by  mmaakkee--bbiigg--llmm.
       Kneser-Ney  smoothing  also  requires  enough disk space to compute and
       store the modified lower-order counts used by the KN method.   This  is
       done using the mmeerrggee--bbaattcchh--ccoouunnttss command, and the --mmaaxx--ppeerr--ffiillee option
       controls how many counts are to be stored per batch, and should be cho-
       sen so that these batches fit in real memory.  The --nnggrraamm--ffiilltteerr option
       allows specification of a command through which the input N-gram counts
       are piped, e.g., to convert from some non-standard format.

       mmaakkee--kknn--ccoouunnttss  computes the modified lower-order counts used by the KN
       smoothing method.  It is invoked as a helper scripts by mmaakkee--bbiigg--llmm ..

       rreeppllaaccee--uunnkk--wwoorrddss replaces words not appearing in the _v_o_c_a_b  file  with
       the unknown word tag <<uunnkk>>.  This is useful for preparing text data for
       LM training.  Only the first token on each line in the  _v_o_c_a_b  file  is
       significant, so both word lists and unigram count files may be used.

       rreeppllaaccee--wwoorrddss--wwiitthh--ccllaasssseess replaces expansions of word classes with the
       corresponding class labels.   _c_l_a_s_s_e_s  specifies  class  expansions  in
       ccllaasssseess--ffoorrmmaatt(5).   Substitutions  are performed at each word position
       in left to right order, with the longest  matching  right-hand-side  of
       any  class  expansion.  If several classes match a pseudo-random choice
       is made.  Optionally, the file _c_o_u_n_t_s will receive the expansion counts
       resulting  from  the  replacements.  nnoorrmmaalliizzee==00 or 11 indicates whether
       the counts should be normalized to probabilities (default is  1).   The
       aaddddoonnee  option  may  be  used  to smooth the expansion probabilities by
       adding _K to each count (default 1).  The option hhaavvee__ccoouunnttss==11 indicates
       that the input consists of N-gram counts and that replacement should be
       performed on them.  Note this will not  merge  counts  that  have  been
       mapped  to  identical  N-grams,  since  this is done automatically when
       nnggrraamm--ccoouunntt(1) reads count data.  The option ppaarrttiiaall==11 prevents  multi-
       word  class  expansions  from  being  replaced when more than one space
       character occurs inbetween the words.

       uunniiffoorrmm--ccllaasssseess takes a file  in  ccllaasssseess--ffoorrmmaatt(5)  and  adds  uniform
       probabilities  to  expansions  that don't have a probability explicitly
       stated.

       mmaakkee--ddiiaaccrriittiicc--mmaapp constructs a map file that pairs an ASCII-fied  ver-
       sion of the words in _v_o_c_a_b with all the occurring non-ASCII word forms.
       Such a map file can then be used with ddiissaammbbiigg(1) and a language  model
       to  reconstruct  the  non-ASCII word form with diacritics from an ASCII
       text.

       vvpp22tteexxtt is a reimplementation of the filter used in the DARPA Hub-3 and
       Hub-4  CSR  evaluations  to convert ``verbalized punctuation'' texts to
       language model training data.

       ccoommppuuttee--oooovv--rraattee determines the out-of-vocabulary rate of a corpus from
       its unigram _c_o_u_n_t_s and a target vocabulary list in _v_o_c_a_b.

SSEEEE AALLSSOO
       ngram-count(1),   ngram(1),   classes-format(5),  disambig(1),  select-
       vocab(1).
       W. Wang, A. Stolcke, J. Zheng, ``Reranking machine translation hypothe-
       ses  with  structured  and web-based language models'', _P_r_o_c_. _I_E_E_E _A_S_R_U
       _W_o_r_k_s_h_o_p, pp. 159-164, 2007.

BBUUGGSS
       Some of the tools could be generalized and/or made more robust to  mis-
       use.
       Several  of  these  tools  are gawk scripts and depending on prevailing
       locale settings might require an LC_NUMERIC=C environment variable.

AAUUTTHHOORR
       Andreas Stolcke <stolcke@icsi.berkeley.edu>
       Copyright (c) 1995-2008 SRI International
       Copyright (c) 2013-2017 Andreas Stolcke
       Copyright (c) 2013-2017 Microsoft Corp.



SRILM Tools              $Date: 2019/09/09 22:35:37 $      training-scripts(1)
